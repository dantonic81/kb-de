![Build](https://img.shields.io/badge/build-passing-brightgreen)
![Docker](https://img.shields.io/badge/docker-ready-blue)
![Dagster](https://img.shields.io/badge/orchestrator-dagster-purple)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

# ğŸ§¬ Data Platform Project


## Table of Contents

- [TL;DR](#-tldr)
- [Introduction](#introduction)
- [Architecture Overview](#-architecture-overview)
  - [System Architecture Diagram](#system-architecture)
  - [Key Components](#key-components)
- [Project Prerequisites](#prerequisites)
- [Configuration Details](#configuration-details)
- [Project Structure](#project-structure)
- [Project Setup](#project-setup)
- [Dagster Job Orchestration & Workflow Overview ](#dagster-job-orchestration--workflow-overview)
- [ETL Pipeline](#etl-pipeline)
  - [ETL Flow Description](#etl-flow-description)
  - [Data Validation Rules](#data-validation-rules)
  - [Error Handling](#error-handling)
- [Testing](#testing)
- [Project Cleanup](#cleanup)
- [Database Schema](#database-schema)
  - [Overview](#overview)
  - [Entity Descriptions](#entity-descriptions)
    - [Patient](#1-patient)
    - [Biometric](#2-biometric)
    - [Patient Biometric Hourly Summary](#3-patientbiometrichourlysummary)
    - [Biometric Trend](#4-biometrictrend)
  - [Schema Validation](#schema-validation)
- [Analytics Engine (Dagster)](#analytics-engine-dagster)
  - [Workflow Overview](#workflow-overview)
- [API Endpoints](#api-endpoints)
- [API Schema and Validation Overview](#api-schema-and-validation-overview)
- [API Documentation](#api-documentation)
- [Functional and Non-Functional Requirements](#-functional-and-non-functional-requirements)
  - [Functional Requirements](#functional-requirements)
  - [Non-Functional Requirements](#non-functional-requirements)
- [Key Design Decisions](#key-design-decisions)
- [Future Improvements](#future-improvements)
- [Limitations](#limitations)
- [Bonus Feature - Kubernetes Orchestration](#bonus-feature---kubernetes-orchestration)
  - [Stack Components](#-stack-components)
  - [Prerequisites](#-prerequisites)
  - [Usage Instructions](#-usage-instructions)
- [Glossary](#-glossary)
- [Acknowledgments](#-acknowledgments)


## âš¡ TL;DR

A production-grade prototype of a health data backend. It includes:
- ETL pipelines with validation and transformation
- Analytics and trend detection
- FastAPI endpoints for querying biometrics and patients
- Dagster for orchestration and observability
- Dockerized and easy to spin up
- Kubernetes deployment as an option


## Introduction

This project demonstrates a backend data integration system designed to ingest, process, and serve biometric health data from healthcare providers and connected devices. It simulates a core component of a patient care platform, focusing on data quality, accessibility, and analytical insight.

Built as a full-stack prototype, it showcases backend engineering skills including orchestration (Dagster), validation (Pydantic, Cerberus), API development (FastAPI), and analytics engineering (dbt, Pandas).

**The system is composed of:**

- **ETL Pipeline:** Extracts structured and unstructured health data, performs validation, normalization, and transformation, and loads it into a database.

- **RESTful API:** Built with FastAPI, this service exposes endpoints for accessing patient and biometric data, supporting CRUD operations and analytical queries.

- **Database Layer:** PostgreSQL schema designed for efficient storage and querying of patient records, biometric readings, and analytical metrics.

- **Analytics Engine:** Hourly cron job processes biometric readings to generate derived metrics (min, max, avg), optimized for large datasets via batch processing.

- **Validation & Error Handling:** Robust handling of missing values, invalid formats, and outliers ensures the pipeline is resilient to real-world data inconsistencies.

- **Job Orchestration & Observability:** Managed with Dagster, offering UI-based visibility and control over scheduled and ad-hoc jobs.

- **Infrastructure:** Containerized using Docker for portability and local orchestration.


**This project can support various use cases like:**

- Continuous health monitoring for clinics

- Dashboards for biometric trend analytics

- Alerts for abnormal biometric readings


**Technologies used:**

- Python 3.11+

- FastAPI

- SQLAlchemy

- Alembic (for database migrations)

- Pandas

- Docker

- Dagster


This solution is designed with maintainability, scalability, and observability in mind, reflecting engineering best practices within a constrained scope.

## ğŸ§± Architecture Overview

### System Architecture

![img1][diagram]


### Key Components


| Service          | Description                                                             |
|------------------|-------------------------------------------------------------------------|
| `db`             | PostgreSQL 15 database used as the central data store.                  |
| `migrate`        | One-shot container that applies database schema migrations via Alembic. |
| `pgadmin`        | Web-based admin interface for PostgreSQL. (optional)                    |
| `api`            | FastAPI application exposing HTTP endpoints.                            |
| `dagster`        | Dagster webserver for viewing and managing data pipelines.              |
| `dagster-daemon` | Background scheduler and job runner for Dagster.                        |



## Prerequisites

- Python
- Docker
- Docker [Compose](https://docs.docker.com/compose/install/linux/#install-using-the-repository) 
- .env file with required environment variables (provided to you via email)



**Makefile Commands:** 

| Command     | Description                                  |
|-------------|----------------------------------------------|
| `make up`   | Builds and starts all services               |
| `make down` | Stops and removes services and containers    |
| `make logs` | Shows logs for all running containers        |
| `make test` | Runs the test suite                          |


## Configuration Details

PostgreSQL database configuration is specified in the .env file.

    .env file example:

    ENV=
    DATABASE_URL=
    DAGSTER_PG_URL=
    POSTGRES_USER=
    POSTGRES_PASSWORD=
    POSTGRES_DB=
    POSTGRES_HOST=
    POSTGRES_PORT=
    PGADMIN_DEFAULT_EMAIL=
    PGADMIN_DEFAULT_PASSWORD=


## Project Structure

```
ğŸ“ project-root/
â”‚
â”œâ”€â”€ğŸ“ alembic/                   # Database migration scripts
â”‚   â”œâ”€â”€ env.py                   # Migration environment config
â”‚   â”œâ”€â”€ script.py.mako           # Migration script template
â”‚   â””â”€â”€ğŸ“ versions/              # Generated migration scripts
â”‚       â”œâ”€â”€ 8002a82dd77c_...     # Specific migration
â”‚       â””â”€â”€ d5233a8698da_...     # Initial migration
â”‚
â”œâ”€â”€ğŸ“ app/                       # Main application code
â”‚   â”œâ”€â”€ğŸ“ analytics/             # Analytics processing
â”‚   â”‚   â”œâ”€â”€ analytics.py         # Core analytics logic
â”‚   â”‚   â””â”€â”€ trend_analyzer.py    # Trend analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ğŸ“ api/                   # API endpoints
â”‚   â”‚   â”œâ”€â”€ biometrics.py        # Biometrics API
â”‚   â”‚   â””â”€â”€ patients.py          # Patients API
â”‚   â”‚
â”‚   â”œâ”€â”€ğŸ“ core/                  # Core application setup
â”‚   â”‚   â””â”€â”€ config.py            # Configuration management
â”‚   â”‚
â”‚   â”œâ”€â”€ğŸ“ db/                    # Database interactions
â”‚   â”‚   â”œâ”€â”€ base.py              # Base database models
â”‚   â”‚   â”œâ”€â”€ models.py            # Data models
â”‚   â”‚   â””â”€â”€ session.py           # Database session handling
â”‚   â”‚
â”‚   â”œâ”€â”€ğŸ“ etl/                   # ETL processes
â”‚   â”‚   â””â”€â”€ run_etl.py           # ETL pipeline execution
â”‚   â”‚
â”‚   â”œâ”€â”€ğŸ“ schemas/               # Data validation schemas
â”‚   â”‚   â”œâ”€â”€ biometric.py         # Biometric schemas
â”‚   â”‚   â”œâ”€â”€ patient.py           # Patient schemas
â”‚   â”‚   â””â”€â”€ ...                  # Other schema definitions
â”‚   â”‚
â”‚   â””â”€â”€ main.py                  # Application entry point
â”‚
â”œâ”€â”€ğŸ“ dagster_home/              # Dagster data orchestration
â”‚   â”œâ”€â”€ *.py                     # Dagster pipeline definitions
â”‚   â”œâ”€â”€ dagster.yaml             # Dagster configuration
â”‚   â”œâ”€â”€ event_logs/              # Execution logs
â”‚   â””â”€â”€ workspace.yaml           # Workspace configuration
â”‚
â”œâ”€â”€ğŸ“ data/                      # Data files and generators
â”‚   â”œâ”€â”€ biometrics.csv           # Sample biometric data
â”‚   â”œâ”€â”€ patients.json            # Patient records
â”‚   â””â”€â”€ time_series_simulator.py # Data generator
â”‚
â”œâ”€â”€ğŸ“ tests/                     # Test suite
â”‚   â”œâ”€â”€ğŸ“ analytics/             # Analytics tests
â”‚   â”œâ”€â”€ğŸ“ api/                   # API tests
â”‚   â”‚   â””â”€â”€ v1/                  # API version tests
â”‚   â”œâ”€â”€ğŸ“ integration/           # Integration tests
â”‚   â””â”€â”€ğŸ“ unit/                  # Unit tests
â”‚
â”œâ”€â”€ğŸ“ etl_output/                # ETL process outputs
â”‚   â””â”€â”€ invalid_*.csv/json       # Invalid records
â”‚
â”œâ”€â”€ docker-compose.yml           # Container orchestration
â”œâ”€â”€ Dockerfile                   # Primary container definition
â”œâ”€â”€ Dockerfile.dagster           # Dagster-specific container
â”œâ”€â”€ Makefile                     # Build automation
â”œâ”€â”€ requirements.txt             # Production dependencies
â”œâ”€â”€ requirements-dev.txt         # Development dependencies
â”œâ”€â”€ setup.py                     # Package configuration
â””â”€â”€ README.md                    # Project documentation
```


## Project Setup

- Clone the repo 
- Navigate to the project directory:

      cd path-to-your-cloned-repo

- Build and run the Docker-compose project: 

      make up

_Note: On slower connections, the initial Docker build may take several minutes â€” nowâ€™s a good time for a coffee refill._

_Also, I've provided a working **pgAdmin** service (credentials provided in the .env file), so if you need it, make sure to uncomment it in **docker-compose.yml** before running the following command_

    

    

This will:

- Start the PostgreSQL database (db) and wait for it to become healthy.

- Launch pgadmin (if uncommented) for optional database inspection via a web UI on http://localhost:8080.

- Run the migrate service to apply Alembic migrations and initialize the database schema.

- Start the FastAPI app (api), which exposes endpoints on http://localhost:8000.

- Bring up the Dagster webserver (dagster), available at http://localhost:3000, for monitoring and running pipelines.

- Start the Dagster daemon (dagster-daemon), responsible for executing scheduled jobs and sensors.

Each service is coordinated using **depends_on** conditions to ensure proper startup order, particularly around the health of the database and the completion of migrations.


## Dagster Job Orchestration & Workflow Overview

Dagster UI is available at [http://localhost:3000](http://localhost:3000)

- Use the `overview` or `jobs` tab to explore pipeline components.
- Trigger ETL or analytics jobs manually from the UI.
- The Dagster daemon automatically executes scheduled jobs every hour.


**The patient biometrics analysis system comprises four key batch jobs executed in a logical sequence:**

1. Run `simulate_time_series_job` to generate synthetic biometrics data as timestamped CSV files.


2. Run `etl_job` to extract, validate, and load this data into the database (`patients` and `biometrics` tables).


3. Run `aggregate_biometrics_job` to compute hourly summaries and populate the `patient_biometric_hourly_summary` table.


4. Run `trend_analyzer_job` to classify biometric trends and store them in the `biometric_trends` table.


| Job                         | Description                                                                                                                                                                           |
|-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `simulate_time_series_job`  | Simulates hourly biometric data (glucose, weight, blood pressure) per patient and writes it to timestamped CSV files for downstream processing.                                       |
| `etl_job`                   | Extracts, validates, and normalizes patient and biometric data from CSV files. Clean data is upserted into a PostgreSQL database, and invalid records are logged for review.          |
| `aggregate_biometrics_job`  | Aggregates biometric readings by patient and hour using statistical summaries. Results are stored in the patient_biometric_hourly_summary table for efficient querying and analysis.  |
| `trend_analyzer_job`        | Analyzes recent biometric patterns (e.g., increasing, stable, volatile), classifies trends for each patient, and stores them in the biometric_trends table.                           |


See [`trend_analyzer_job`](app/analytics/trend_analyzer.py) for implementation of this bonus feature.

## ETL Pipeline

### ETL Flow Description

The ETL pipeline is defined in `app/etl/run_etl.py` and follows this flow:

1. Load source data from `data/` (CSV/JSON).
2. Validate using Pandera schemas.
3. Normalize timestamps, types, units.
4. Store valid records in PostgreSQL.
5. Write invalid records to `etl_output/`.

Run manually with:

    python app/etl/run_etl.py

### Data Validation Rules

Both patient and biometric data go through a validation stage before being processed.

| Field            | Validation Rule                                                         |
|------------------|-------------------------------------------------------------------------|
| Patient Email    | Must be in a valid email format (`.*@.*\..*`)                           |
| Biometric Values | Must fall within accepted medical ranges (e.g., glucose: 70â€“200 mg/dL)  |
| Timestamps       | Must be in ISO 8601 format                                              |



### Error Handling

- **Invalid data** â†’ Logged to `etl_output/invalid_*.csv` for review.
- **Missing fields** â†’ Either filled with defaults or rejected depending on context.
- **Outliers** â†’ Marked with `is_outlier=True` for downstream handling.



## Testing (Bonus Feature)

It's recommended to use a separate **virtual environment** when working with this project. While setting up a virtual environment is outside the scope of this document, once it's activated, install the development dependencies:


    pip install -r requirements-dev.txt


Then, to run the full test suite:
   
    make test

or manually:

    pytest tests/


## Cleanup

Once you no longer need these containers running and you no longer need the data either, simply turn the whole thing off by running the following:

    make down

_ğŸ’¡ Note: This will stop and remove the containers, but volumes (e.g., database data) may persist unless explicitly configured otherwise in docker-compose.yml._





## Database Schema

### Overview

The system is centered around patients and their biometric data, including both raw measurements and summarized or analyzed trends. The database schema has been designed with normalization in mind, ensuring data consistency, referential integrity, and analytical flexibility.

### Entity Descriptions

#### 1. Patient

This table stores personal information about each patient.

- Primary Key: id

- Unique Constraint: email

- Relationships:

  - One-to-many with Biometric

  - One-to-many with PatientBiometricHourlySummary

  - One-to-many with BiometricTrend

#### 2. Biometric

Stores raw biometric measurements.

- Primary Key: id

- Foreign Key: patient_id â†’ Patient.id

- Composite Unique Constraint: (patient_id, biometric_type, timestamp)

- Fields allow storing both scalar (e.g., weight, glucose) and compound (e.g., blood pressure systolic/diastolic) values.

#### 3. PatientBiometricHourlySummary

Captures hourly aggregates of biometric data.

- Primary Key: id

- Foreign Key: patient_id â†’ Patient.id

- Composite Unique Constraint: (patient_id, biometric_type, hour_start)

- Aggregated values include min_value, max_value, avg_value, and count.


#### 4. BiometricTrend

Stores analyzed trends derived from biometric data.

- Primary Key: id

- Foreign Key: patient_id â†’ Patient.id

- Composite Unique Constraint: (patient_id, biometric_type)

- trend is a categorical field indicating the nature of biometric changes (e.g., increasing, decreasing, stable).


### Schema Validation

Two Pandera schemas ensure data integrity for incoming data pipelines:

**BiometricSchema:** Ensures valid biometric input with regex pattern checks and biometric type validation.

**PatientSchema:** Enforces presence and type of core patient information fields.


## Analytics Engine (Dagster)

The analytics pipeline is orchestrated by Dagster and runs hourly via the dagster-daemon.


### Workflow Overview:


- Input: Pulls biometric readings from the past hour.

- Processing:

  - Computes min, max, and average values per patient and biometric type.

  - Analyzes trends using moving averages:

    - Classified as improving, stable, or declining.

- Output: Writes results to patient_biometric_hourly_summary table in the database.




## API Endpoints


| Endpoint                             | Method | Description                                            |
|--------------------------------------|--------|--------------------------------------------------------|
| `/patients`                          | GET    | List patients (paginated)                              |
| `/patients/{id}/biometrics`          | GET    | Get biometric history (filter by `?type=glucose`)      |
| `/biometrics`                        | POST   | Upsert a biometric reading                             |
| `/biometrics/{id}`                   | DELETE | Delete a biometric record by its ID                    |
| `/biometrics/{patient_id}/analytics` | GET    | Retrieve hourly biometric stats for a specific patient |


**Example Request:**

    curl "http://localhost:8000/biometrics/1?type=glucose&limit=2" | jq    

**Example Response:**

    {
      "data": [
        {
          "biometric_type": "glucose",
          "timestamp": "2025-06-02T23:00:00",
          "unit": "mg/dL",
          "value": 117.0,
          "systolic": null,
          "diastolic": null,
          "id": 1051,
          "patient_id": 1
        },
        {
          "biometric_type": "glucose",
          "timestamp": "2025-06-02T22:00:00",
          "unit": "mg/dL",
          "value": 123.0,
          "systolic": null,
          "diastolic": null,
          "id": 1036,
          "patient_id": 1
        }
      ],
      "total": 71,
      "skip": 0,
      "limit": 2
    }


## API Schema and Validation Overview

This project uses Pydantic models to define and validate API data structures.

To keep our API data consistent and reliable, we use a system that defines clear rules for the kind of information we accept and return.

We specify what fields are required, what types of data are allowed, and when certain details are necessary. For example, some measurements need extra details only if they belong to a specific category, like blood pressure.

The system automatically checks incoming data against these rules, so incorrect or incomplete information is caught early.

It also formats the data properly when sending it back, making sure things like dates are easy to understand.

This approach helps prevent errors, ensures data quality, and makes the whole API smoother to use and maintain.


## API Documentation


API Documentation is handled by Built-in API Docs (Swagger / ReDoc)


Swagger UI: http://localhost:8000/docs

ReDoc: http://localhost:8000/redoc


## âœ… Functional and Non-Functional Requirements


### Functional Requirements


- Ingest biometric and patient data via ETL

- Expose CRUD API endpoints for patient and biometric data

- Aggregate biometrics hourly per patient

- Analyze trends in patient biometrics (e.g., increasing/decreasing)

- Provide job orchestration and scheduling via Dagster

### Non-Functional Requirements

- Scalability: Batch-processing of large datasets and aggregation jobs

- Resilience: ETL handles missing values, malformed data, and schema violations gracefully

- Observability: Dagster UI provides operational visibility; logs invalid data records

- Maintainability: Modular codebase, use of SQLAlchemy, Alembic, Docker

- Portability: Docker-based deployment with .env for environment separation

- Testability: Structured test suite for unit and integration testing

- Extensibility: Schema and job definitions are adaptable for new sources or metrics


## Key Design Decisions

- **Idempotent ETL** â€” Retry-safe pipeline to prevent duplicate records.

- **Batch Processing** â€” Uses pandas for efficient in-memory transformations.

- **Trend Analysis** â€” Relies on simple moving averages to detect stability or changes.

- **Containerization** â€” Docker-based architecture for modular and scalable deployment.


## Future Improvements

- **Stream Processing with Kafka/Flink** â€” for real-time data ingestion and processing to reduce latency adn support event-driven workflows. This will allow for timely anomaly detection and dynamic dashboards.

- **GraphQL API Layer** â€” Provide more flexible, client-driven queries to reduce over-fetching and under-fetching of data, improving client performance and usability.

- **Anomaly Detection using ML** â€” Integrate machine learning models to automatically detect outliers in biometric data, improving clinical insights and alerting.

- **Multi-Tenant Support** â€” Architect the system to isolate data and configurations per client, enabling SaaS deployments and ensuring data privacy.

- **Integration with Visualization Tools (Metabase, Superset, Grafana)** â€” Provide rich, interactive dashboards for business users and clinicians, enhancing data exploration without coding.

- **CI/CD Pipelines for Automated Testing and Deployment** â€” Ensure rapid and safe iteration cycles with automated tests, container builds, and deployment to Kubernetes or cloud environments.

## Limitations

- **No Authentication or Authorization** â€” The API is unsecured, suitable only for local demos. Production deployments require integration with identity providers and RBAC.

- **Unproven Production Scalability** â€” The system has not been load-tested for concurrent writes or high volume patient data, which may cause performance degradation or failures.

- **Lack of Horizontal Scalability** â€” Current architecture is monolithic and may not scale well horizontally; Kubernetes deployment is a step toward microservices and better scaling.

- **Static Schema Assumption** â€” Schema changes require manual migration; no support for dynamic schema evolution or versioning yet.

- **Synthetic Data Only** â€” The system currently uses mock data; real-world validation with actual patient data is necessary to ensure accuracy and compliance.

- **Local Data Persistence by Default** â€” Data is stored locally unless explicitly configured to use external storage, limiting durability and fault tolerance.

- **Scheduler Assumes System Time Sync** â€” Timing-based orchestrations depend on synchronized system clocks across services; lack of NTP or time drift handling could cause scheduling errors.



## CI/CD

This project is designed for local development. Deployment to cloud infrastructure (e.g., AWS, GCP) is out of scope but could be integrated with Terraform and CI pipelines.


## Bonus Feature - Kubernetes Orchestration


This feature includes Kubernetes manifests under the `k8s/` directory to orchestrate the application stack locally using Minikube.

### ğŸ“¦ Stack Components

- FastAPI application

- PostgreSQL database

- Dagster orchestration UI

- CronJob for biometric analytics

### âœ… Prerequisites

- [Minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download) installed and running locally

- [Docker](https://docs.docker.com/get-started/get-docker/) installed and accessible from the terminal

- Start Minikube (if not already running):

      minikube start

### ğŸ›  Usage Instructions

ğŸ’¡ Tip: Before running the Kubernetes setup, shut down any Docker Compose services to avoid conflicts:


    docker compose down


1. Navigate to the Kubernetes directory:

       cd k8s

2. Build Docker images inside the Minikube environment:

       make build

3. Deploy all Kubernetes resources:

       make deploy

4. Access the Dagster UI:

       make open-dagster

This opens http://localhost:3000 in your browser.


5. Access the FastAPI Swagger UI:

        make open-swagger

6. Clean up all Kubernetes resources:

        make clean

7. (Optional) Delete the Minikube cluster entirely:

        minikube delete









<!-- link label -->
[diagram]: Screenshots/Diagram.PNG "System Architecture Diagram"


## ğŸ“˜ Glossary

| Term                        | Description                                              |
|-----------------------------|----------------------------------------------------------|
| ETL                        | Extract, Transform, Load â€“ classic data pipeline pattern |
| Dagster                    | Data orchestrator tool with modern UI and scheduling     |
| Pydantic                   | Python library for data validation using type hints      |
| Outlier                    | Biometric reading outside expected physiological bounds  |

## Other packages used (not listed in requirements)

- Black
- Flake8


## ğŸ™ Acknowledgments

Special thanks to:
- Eric, for providing the project requirements and valuable feedback.

---
_Developed and tested on Ubuntu 24.10 (2025)_